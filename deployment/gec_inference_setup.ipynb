{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEC Model Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture insall_out --no-stderr\n",
    "\n",
    "!pip install huggingface_hub transformers boto3 sagemaker wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.197.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.199.0.tar.gz (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (23.1.0)\n",
      "Collecting boto3<2.0,>=1.33.3 (from sagemaker)\n",
      "  Downloading boto3-1.33.6-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.26.1)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.24.4)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.5.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.3.1)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: PyYAML~=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.0.1)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.19.1)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (3.11.0)\n",
      "Requirement already satisfied: tblib==1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.7.0)\n",
      "Requirement already satisfied: urllib3<1.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.26.18)\n",
      "Collecting uvicorn==0.22.0 (from sagemaker)\n",
      "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastapi==0.95.2 (from sagemaker)\n",
      "  Downloading fastapi-0.95.2-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (2.31.0)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.1.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.66.1)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (5.9.5)\n",
      "Collecting pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 (from fastapi==0.95.2->sagemaker)\n",
      "  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi==0.95.2->sagemaker)\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from uvicorn==0.22.0->sagemaker) (8.1.7)\n",
      "Collecting h11>=0.8 (from uvicorn==0.22.0->sagemaker)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting botocore<1.34.0,>=1.33.6 (from boto3<2.0,>=1.33.3->sagemaker)\n",
      "  Downloading botocore-1.33.6-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.33.3->sagemaker) (1.0.1)\n",
      "Collecting s3transfer<0.9.0,>=0.8.2 (from boto3<2.0,>=1.33.3->sagemaker)\n",
      "  Downloading s3transfer-0.8.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.17.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.1.1)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from docker->sagemaker) (1.6.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->sagemaker) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->sagemaker) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->sagemaker) (2023.7.22)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2023.3.post1)\n",
      "Requirement already satisfied: ppft>=1.7.6.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.7)\n",
      "Requirement already satisfied: dill>=0.3.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.7)\n",
      "Requirement already satisfied: pox>=0.3.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.15 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.15)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from schema->sagemaker) (21.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi==0.95.2->sagemaker) (4.8.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (4.0.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi==0.95.2->sagemaker) (1.1.3)\n",
      "Downloading fastapi-0.95.2-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading boto3-1.33.6-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading botocore-1.33.6-py3-none-any.whl (11.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading s3transfer-0.8.2-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: sagemaker\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.199.0-py2.py3-none-any.whl size=1374440 sha256=a1c2a555a88ab4bafde37c71e5ed7d3e1c964dffc73aed3d599adb84d02e7d81\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/0b/36/29/905c1d229d6d1e782514dfda3d24640d41953098bb3061ad6c\n",
      "Successfully built sagemaker\n",
      "Installing collected packages: pydantic, h11, uvicorn, starlette, botocore, s3transfer, fastapi, boto3, sagemaker\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.4.2\n",
      "    Uninstalling pydantic-2.4.2:\n",
      "      Successfully uninstalled pydantic-2.4.2\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.31.85\n",
      "    Uninstalling botocore-1.31.85:\n",
      "      Successfully uninstalled botocore-1.31.85\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.7.0\n",
      "    Uninstalling s3transfer-0.7.0:\n",
      "      Successfully uninstalled s3transfer-0.7.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.28.85\n",
      "    Uninstalling boto3-1.28.85:\n",
      "      Successfully uninstalled boto3-1.28.85\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.197.0\n",
      "    Uninstalling sagemaker-2.197.0:\n",
      "      Successfully uninstalled sagemaker-2.197.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.29.85 requires botocore==1.31.85, but you have botocore 1.33.6 which is incompatible.\n",
      "awscli 1.29.85 requires s3transfer<0.8.0,>=0.7.0, but you have s3transfer 0.8.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.33.6 botocore-1.33.6 fastapi-0.95.2 h11-0.14.0 pydantic-1.10.13 s3transfer-0.8.2 sagemaker-2.199.0 starlette-0.27.0 uvicorn-0.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install transformers==4.28.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture insall_out --no-stderr\n",
    "\n",
    "# !pip install --upgrade sagemaker wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer #, AutoModelForTokenClassification\n",
    "from transformers.models.auto.modeling_auto import AutoModelForTokenClassification\n",
    "from transformers import TokenClassificationPipeline\n",
    "import boto3 \n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import pipeline\n",
    "import sagemaker\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "from sagemaker.huggingface import HuggingFaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.35.2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.197.0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "571667364805 us-west-2 arn:aws:iam::571667364805:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole\n"
     ]
    }
   ],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "\n",
    "# The name of our algorithm -- i.e. the name of the inference container\n",
    "INFERENCE_ALGORITHM_NAME = \"sm-gec-aws\"\n",
    "ENDPOINT_NAME = \"sm-gec-aws\"\n",
    "IMAGE_URI_INFERENCE = (\n",
    "    f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{INFERENCE_ALGORITHM_NAME}:latest\"\n",
    ")\n",
    "HF_MODEL_ID = 'ramsenth/langbot-gec'\n",
    "HF_TASK = 'token-classification'\n",
    "instance_type = \"ml.m5.large\" #\"ml.p2.xlarge\"\n",
    "\n",
    "# see deep learning containers (DLC) available images here:\n",
    "# https://github.com/aws/deep-learning-containers/blob/master/available_images.md \n",
    "MODEL_IMAGE_URL=f'763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04'\n",
    "# model_image_url=f'763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.25.0-deepspeed0.11.0-cu118'\n",
    "\n",
    "print(account_id, region, role)\n",
    "\n",
    "# session = boto3.Session(profile_name=profile_name)\n",
    "session = boto3.Session()\n",
    "sm_client = session.client('sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Try HuggingFace Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(HF_TASK, model=HF_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-na',\n",
       "  'score': 0.62121904,\n",
       "  'index': 6,\n",
       "  'word': 'tiempos',\n",
       "  'start': 18,\n",
       "  'end': 25}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe('Sí, tengo algo de tiempos hoy.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-na',\n",
       "  'score': 0.62121904,\n",
       "  'index': 6,\n",
       "  'word': 'tiempos',\n",
       "  'start': 18,\n",
       "  'end': 25}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_ID)\n",
    "model = AutoModelForTokenClassification.from_pretrained(HF_MODEL_ID)\n",
    "\n",
    "pipe = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "pipe(\"Sí, tengo algo de tiempos hoy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Endpoints': [], 'ResponseMetadata': {'RequestId': '9d05280b-b21c-4892-9ceb-2baec58f32d5', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '9d05280b-b21c-4892-9ceb-2baec58f32d5', 'content-type': 'application/x-amz-json-1.1', 'content-length': '16', 'date': 'Wed, 29 Nov 2023 23:49:39 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "# session = boto3.Session(profile_name=profile_name)\n",
    "session = boto3.Session()\n",
    "sm_client = session.client('sagemaker', region_name=region)\n",
    "response = sm_client.list_endpoints()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Model Tar File For Deployment\n",
    "Before we can deploy we need to prepare the tar file. Steps:\n",
    "* Download model artifacts from WandB\n",
    "* Add the custom inference script\n",
    "* Create tar file\n",
    "* Upload tarfile to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# WandB artifact id\n",
    "WANDB_ARTIFACT='langbot/langbot_gec_plain_top_performers/model-beto_pytorch_final:latest'\n",
    "S3_ARTIFACT = 's3://project-langbot-models/gec-simple-model-hf-pytorch-custom-infer.tar.gz'\n",
    "S3_WORKING_ARTIFACT = 's3://project-langbot-models/gec-simple-model-hf-pytorch-custom-infer-working.tar.gz'\n",
    "\n",
    "def download_model(artifact_id):\n",
    "    run = wandb.init()\n",
    "    artifact = run.use_artifact(artifact_id, type='model')\n",
    "    artifact_dir = artifact.download()\n",
    "    wandb.finish()\n",
    "    return artifact_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ec2-user/SageMaker/model_gec_ram/deployment/wandb/run-20231204_190928-ego4cous</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/langbot/model_gec_ram-deployment/runs/ego4cous' target=\"_blank\">lucky-shadow-1</a></strong> to <a href='https://wandb.ai/langbot/model_gec_ram-deployment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/langbot/model_gec_ram-deployment' target=\"_blank\">https://wandb.ai/langbot/model_gec_ram-deployment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/langbot/model_gec_ram-deployment/runs/ego4cous' target=\"_blank\">https://wandb.ai/langbot/model_gec_ram-deployment/runs/ego4cous</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-beto_pytorch_final:latest, 417.77MB. 7 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   7 of 7 files downloaded.  \n",
      "Done. 0:0:10.8\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lucky-shadow-1</strong> at: <a href='https://wandb.ai/langbot/model_gec_ram-deployment/runs/ego4cous' target=\"_blank\">https://wandb.ai/langbot/model_gec_ram-deployment/runs/ego4cous</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231204_190928-ego4cous/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "artifact_dir = download_model(WANDB_ARTIFACT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-na',\n",
       "  'score': 0.8623703,\n",
       "  'index': 6,\n",
       "  'word': 'tiempos',\n",
       "  'start': 18,\n",
       "  'end': 25}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the downloaded model\n",
    "def test(model_loc):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_loc)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_loc)\n",
    "    pipe = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "    return pipe('Sí, tengo algo de tiempos hoy.')\n",
    "\n",
    "test(artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the code folder and add the inference script\n",
    "!mkdir {artifact_dir}/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, TokenClassificationPipeline\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "\n",
    "JSON_CONTENT_TYPE = 'application/json'\n",
    "THRESHOLD = 0.3\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    # implement custom code to load the model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_dir)\n",
    "    pipe = TokenClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "    logger.info(f\"model_fn:: Loaded model from model_dir={model_dir}\")\n",
    "    return pipe\n",
    "\n",
    "def input_fn(serialized_input_data, content_type=JSON_CONTENT_TYPE):\n",
    "    # decode the input data  (e.g. JSON string -> dict)\n",
    "    if content_type == JSON_CONTENT_TYPE:\n",
    "        input_data = json.loads(serialized_input_data)\n",
    "        return input_data['line']\n",
    "    else:\n",
    "        raise Exception('Requested unsupported ContentType in Accept: ' + content_type)\n",
    "\n",
    "def _translate(data, model_ret):\n",
    "    # Parse model output which has :\n",
    "    # [{'entity': 'B-na',\n",
    "    #   'score': 0.7923039,\n",
    "    #   'index': 6,\n",
    "    #   'word': 'tiempos',\n",
    "    #   'start': 18,\n",
    "    #   'end': 25}]\n",
    "\n",
    "    # Split input to words\n",
    "    words = re.findall(r'\\b\\w+\\b|\\[.*?\\]\\{.*?\\}\\<.*?\\>|\\S', data)\n",
    "    lookup = {}\n",
    "    for cur in model_ret:\n",
    "        score = float(cur['score'])\n",
    "        if score > THRESHOLD:\n",
    "            start = int(cur['start'])\n",
    "            end = int(cur['end'])\n",
    "            word = data[start: end]\n",
    "            cur['matched'] = False\n",
    "            lookup[word] = cur\n",
    "\n",
    "    # print(lookup)\n",
    "    output = []\n",
    "    for word in words:\n",
    "        if word in lookup:\n",
    "            cur['matched'] = True\n",
    "            output.append({word: cur['entity']})\n",
    "        else:\n",
    "            output.append({word: 'O'})\n",
    "    return output\n",
    "        \n",
    "def predict_fn(data, model):\n",
    "    outputs = model(data)\n",
    "    translated = _translate(data, outputs)\n",
    "    for cur in outputs:\n",
    "        cur['score'] = str(cur['score'])\n",
    "        cur['index'] = str(cur['index'])\n",
    "        cur['start'] = str(cur['start'])\n",
    "        cur['end'] = str(cur['end'])\n",
    "    final = {\n",
    "        'result': translated,\n",
    "        'model_response': json.dumps(outputs)\n",
    "    }\n",
    "\n",
    "    return json.dumps(final)\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/model_gec_ram/deployment/artifacts/model-beto_pytorch_final:v14'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./gec-simple-model-hf-pytorch-custom-infer.tar.gz to s3://project-langbot-models/gec-simple-model-hf-pytorch-custom-infer.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Create the tar file and upload to s3\n",
    "!tar -czf gec-simple-model-hf-pytorch-custom-infer.tar.gz --directory={artifact_dir} .\n",
    "!aws s3 cp ./gec-simple-model-hf-pytorch-custom-infer.tar.gz {S3_ARTIFACT}\n",
    "!rm gec-simple-model-hf-pytorch-custom-infer.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/model_gec_ram/deployment\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "----!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.huggingface.model.HuggingFacePredictor at 0x7f2fce863070>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    # model_data=S3_ARTIFACT,  # path to your trained SageMaker model\n",
    "    model_data=S3_WORKING_ARTIFACT,\n",
    "    role=role,                                            # IAM role with permissions to create an endpoint\n",
    "    transformers_version=\"4.6.1\",                           # Transformers version used\n",
    "    pytorch_version=\"1.7.1\",                                # PyTorch version used\n",
    "    py_version='py36',                                    # Python version used\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "huggingface_model.deploy(\n",
    "    endpoint_name = ENDPOINT_NAME,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Test Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Endpoints': [{'EndpointName': 'sm-gec-aws', 'EndpointArn': 'arn:aws:sagemaker:us-west-2:571667364805:endpoint/sm-gec-aws', 'CreationTime': datetime.datetime(2023, 12, 4, 19, 19, 14, 337000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2023, 12, 4, 19, 21, 18, 177000, tzinfo=tzlocal()), 'EndpointStatus': 'InService'}, {'EndpointName': 'sm-llm-aws', 'EndpointArn': 'arn:aws:sagemaker:us-west-2:571667364805:endpoint/sm-llm-aws', 'CreationTime': datetime.datetime(2023, 12, 4, 18, 32, 42, 21000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2023, 12, 4, 18, 37, 23, 50000, tzinfo=tzlocal()), 'EndpointStatus': 'InService'}], 'ResponseMetadata': {'RequestId': '3e9dc959-4c7e-4108-a4ae-ecc36da2aaf7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '3e9dc959-4c7e-4108-a4ae-ecc36da2aaf7', 'content-type': 'application/x-amz-json-1.1', 'content-length': '422', 'date': 'Mon, 04 Dec 2023 19:21:44 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = sm_client.list_endpoints()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'd1cef595-c5c5-46a1-bffb-e492b7f370f7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'd1cef595-c5c5-46a1-bffb-e492b7f370f7', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Mon, 04 Dec 2023 19:21:50 GMT', 'content-type': 'application/json', 'content-length': '266', 'connection': 'keep-alive'}, 'RetryAttempts': 0}, 'ContentType': 'application/json', 'InvokedProductionVariant': 'AllTraffic', 'Body': <botocore.response.StreamingBody object at 0x7f2fce81ead0>}\n",
      "b'{\"result\": [{\"Estoy\": \"O\"}, {\"bienes\": \"B-ga\"}, {\",\": \"O\"}, {\"gracias\": \"O\"}, {\".\": \"O\"}], \"model_response\": \"[{\\\\\"word\\\\\": \\\\\"bienes\\\\\", \\\\\"score\\\\\": \\\\\"0.34608975052833557\\\\\", \\\\\"entity\\\\\": \\\\\"B-ga\\\\\", \\\\\"index\\\\\": \\\\\"2\\\\\", \\\\\"start\\\\\": \\\\\"6\\\\\", \\\\\"end\\\\\": \\\\\"12\\\\\", \\\\\"matched\\\\\": true}]\"}'\n",
      "{'ResponseMetadata': {'RequestId': '44972f26-302b-42b9-aa3c-a52f524169f2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '44972f26-302b-42b9-aa3c-a52f524169f2', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Mon, 04 Dec 2023 19:21:50 GMT', 'content-type': 'application/json', 'content-length': '310', 'connection': 'keep-alive'}, 'RetryAttempts': 0}, 'ContentType': 'application/json', 'InvokedProductionVariant': 'AllTraffic', 'Body': <botocore.response.StreamingBody object at 0x7f2fce81f0a0>}\n",
      "b'{\"result\": [{\"S\\\\u00ed\": \"O\"}, {\",\": \"O\"}, {\"tengo\": \"O\"}, {\"algo\": \"O\"}, {\"de\": \"O\"}, {\"tiempos\": \"B-na\"}, {\"hoy\": \"O\"}, {\".\": \"O\"}], \"model_response\": \"[{\\\\\"word\\\\\": \\\\\"tiempos\\\\\", \\\\\"score\\\\\": \\\\\"0.7923046946525574\\\\\", \\\\\"entity\\\\\": \\\\\"B-na\\\\\", \\\\\"index\\\\\": \\\\\"6\\\\\", \\\\\"start\\\\\": \\\\\"18\\\\\", \\\\\"end\\\\\": \\\\\"25\\\\\", \\\\\"matched\\\\\": true}]\"}'\n",
      "{'ResponseMetadata': {'RequestId': 'e059b91a-9b58-4550-bd61-8393150cf467', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'e059b91a-9b58-4550-bd61-8393150cf467', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Mon, 04 Dec 2023 19:21:50 GMT', 'content-type': 'application/json', 'content-length': '298', 'connection': 'keep-alive'}, 'RetryAttempts': 0}, 'ContentType': 'application/json', 'InvokedProductionVariant': 'AllTraffic', 'Body': <botocore.response.StreamingBody object at 0x7f2fce81f250>}\n",
      "b'{\"result\": [{\"S\\\\u00ed\": \"O\"}, {\",\": \"O\"}, {\"necesito\": \"O\"}, {\"comprar\": \"O\"}, {\"un\": \"B-ga\"}, {\"chaqueta\": \"O\"}, {\".\": \"O\"}], \"model_response\": \"[{\\\\\"word\\\\\": \\\\\"un\\\\\", \\\\\"score\\\\\": \\\\\"0.9915907979011536\\\\\", \\\\\"entity\\\\\": \\\\\"B-ga\\\\\", \\\\\"index\\\\\": \\\\\"5\\\\\", \\\\\"start\\\\\": \\\\\"21\\\\\", \\\\\"end\\\\\": \\\\\"23\\\\\", \\\\\"matched\\\\\": true}]\"}'\n",
      "{'ResponseMetadata': {'RequestId': '84c79f43-8348-46b0-9272-a5ad23699a67', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '84c79f43-8348-46b0-9272-a5ad23699a67', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Mon, 04 Dec 2023 19:21:50 GMT', 'content-type': 'application/json', 'content-length': '90', 'connection': 'keep-alive'}, 'RetryAttempts': 0}, 'ContentType': 'application/json', 'InvokedProductionVariant': 'AllTraffic', 'Body': <botocore.response.StreamingBody object at 0x7f2fce81f4c0>}\n",
      "b'{\"result\": [{\"A\": \"O\"}, {\"las\": \"O\"}, {\"dieza\": \"O\"}, {\".\": \"O\"}], \"model_response\": \"[]\"}'\n",
      "{'ResponseMetadata': {'RequestId': '1e3f4300-e925-40c4-88f6-423cee8e2807', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1e3f4300-e925-40c4-88f6-423cee8e2807', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Mon, 04 Dec 2023 19:21:50 GMT', 'content-type': 'application/json', 'content-length': '80', 'connection': 'keep-alive'}, 'RetryAttempts': 0}, 'ContentType': 'application/json', 'InvokedProductionVariant': 'AllTraffic', 'Body': <botocore.response.StreamingBody object at 0x7f2fce81efb0>}\n",
      "b'{\"result\": [{\"Hasta\": \"O\"}, {\"luega\": \"O\"}, {\".\": \"O\"}], \"model_response\": \"[]\"}'\n",
      "{'ResponseMetadata': {'RequestId': '0aaf5fbe-f444-4512-96bb-2bc7c8dea51c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '0aaf5fbe-f444-4512-96bb-2bc7c8dea51c', 'x-amzn-invoked-production-variant': 'AllTraffic', 'date': 'Mon, 04 Dec 2023 19:21:50 GMT', 'content-type': 'application/json', 'content-length': '38', 'connection': 'keep-alive'}, 'RetryAttempts': 0}, 'ContentType': 'application/json', 'InvokedProductionVariant': 'AllTraffic', 'Body': <botocore.response.StreamingBody object at 0x7f2fce81f490>}\n",
      "b'{\"result\": [], \"model_response\": \"[]\"}'\n"
     ]
    }
   ],
   "source": [
    "# WAIT FOR ENDPOINT TO BE \"IN SERVICE\" BEFORE PROCEEDING WITH THIS STEP\n",
    "\n",
    "# invoke endpoint by endpoint name\n",
    "import json\n",
    "sm_runtime = session.client(\"sagemaker-runtime\", region_name=region)\n",
    "\n",
    "content_type = \"application/json\"\n",
    "\n",
    "def test(text):\n",
    "    # specify \"Inputs\"\n",
    "    data = {\n",
    "       \"line\": text #\"Sí, tengo algo de tiempos hoy.\"\n",
    "    }\n",
    "\n",
    "    response = sm_runtime.invoke_endpoint(\n",
    "        EndpointName = 'sm-gec-aws',\n",
    "        ContentType = content_type,\n",
    "        Body=json.dumps(data)\n",
    "    )\n",
    "    print(response)\n",
    "    print(response[\"Body\"].read())\n",
    "    \n",
    "\n",
    "texts = [\n",
    "    'Estoy bienes, gracias.',\n",
    "    'Sí, tengo algo de tiempos hoy.',\n",
    "    'Sí, necesito comprar un chaqueta.',\n",
    "    'A las dieza.',\n",
    "    'Hasta luega.'\n",
    "]\n",
    "[test(text) for text in texts]\n",
    "\n",
    "test('')\n",
    "# pair 1\n",
    "# Assistant: Hola, ¿cómo estás? (Hello, how are you?)\n",
    "# user_input = \"Estoy bienes, gracias.\"\n",
    "# input_error = \"the word, bienes, has a number disagreement error.\"\n",
    "# next_question = \"¿Estás libre hoy?\"\n",
    "\n",
    "# pair 2\n",
    "# user_input = \"Sí, tengo algo de tiempos hoy.\"\n",
    "# input_error = \"the word, tiempos, has a number disagreement error.\"\n",
    "# next_question = \"¿Quieres ir de compras conmigo?\"\n",
    "\n",
    "# pair 3\n",
    "# user_input = \"Sí, necesito comprar un chaqueta.\"\n",
    "# input_error = \"the word, un, has a gender disagreement error.\"\n",
    "# next_question = \"¿A qué hora te gustaría ir?\"\n",
    "\n",
    "# pair 4\n",
    "# user_input = \"A las dieza.\"\n",
    "# input_error = \"the word, dieza, has a gender disagreement error.\"\n",
    "# next_question = \"Vale, nos vemos luego.\"\n",
    "\n",
    "# pair 5\n",
    "# user_input = \"Hasta luega.\"\n",
    "# input_error = \"the word, luega, has a gender disagreement error.\"\n",
    "# next_question = \"adiós!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **DEPRECATED** Trial 1 - Custom Deploy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sm-gec-aws\n",
      "sm-gec-aws\n",
      "sm-gec-aws\n"
     ]
    }
   ],
   "source": [
    "# set model name and endpoint configuration name\n",
    "import time\n",
    "timestamp = time.strftime('-%Y-%m-%d-%H-%M-%S', time.gmtime())\n",
    "model_name = INFERENCE_ALGORITHM_NAME\n",
    "endpoint_config_name = INFERENCE_ALGORITHM_NAME\n",
    "endpoint_name = INFERENCE_ALGORITHM_NAME\n",
    "print(model_name)\n",
    "print(endpoint_config_name)\n",
    "print(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sm-gec-aws'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\n",
      "{'Image': '763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04', 'ModelDataUrl': 's3://project-langbot-models/gec-simple-model-hf-pytorch.tar.gz', 'Mode': 'SingleModel', 'Environment': {'HF_TASK': 'token-classification', 'SAGEMAKER_CONTAINER_LOG_LEVEL': '20', 'SAGEMAKER_REGION': 'us-west-2'}}\n",
      "{'ModelArn': 'arn:aws:sagemaker:us-west-2:571667364805:model/sm-gec-aws', 'ResponseMetadata': {'RequestId': '59a40e8c-8459-4974-a461-c139ba8e5265', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '59a40e8c-8459-4974-a461-c139ba8e5265', 'content-type': 'application/x-amz-json-1.1', 'content-length': '72', 'date': 'Wed, 29 Nov 2023 05:21:58 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_IMAGE_URL)\n",
    "\n",
    "# set container config\n",
    "container_config = {\n",
    "    'Image': MODEL_IMAGE_URL,\n",
    "    'ModelDataUrl': 's3://project-langbot-models/gec-simple-model-hf-pytorch.tar.gz',\n",
    "    'Mode': 'SingleModel',\n",
    "    'Environment': {\n",
    "        # 'HF_MODEL_ID': HF_MODEL_ID,\n",
    "        'HF_TASK' : HF_TASK,\n",
    "        'SAGEMAKER_CONTAINER_LOG_LEVEL' : '20',\n",
    "        'SAGEMAKER_REGION' : region\n",
    "    }\n",
    "}\n",
    "print(container_config)\n",
    "\n",
    "# create model\n",
    "# ... models console: https://console.aws.amazon.com/sagemaker/home?#/models\n",
    "response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    PrimaryContainer=container_config,\n",
    "    ExecutionRoleArn=role, \n",
    "    EnableNetworkIsolation=False\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EndpointConfigArn': 'arn:aws:sagemaker:us-west-2:571667364805:endpoint-config/sm-gec-aws', 'ResponseMetadata': {'RequestId': '79a27733-b71e-4a4d-ac25-a20400f0c11a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '79a27733-b71e-4a4d-ac25-a20400f0c11a', 'content-type': 'application/x-amz-json-1.1', 'content-length': '91', 'date': 'Wed, 29 Nov 2023 05:22:17 GMT'}, 'RetryAttempts': 0}}\n",
      "Endpoint configuration name: sm-gec-aws\n",
      "Endpoint configuration arn:  arn:aws:sagemaker:us-west-2:571667364805:endpoint-config/sm-gec-aws\n"
     ]
    }
   ],
   "source": [
    "# create endpoint config\n",
    "# ... endpoint configs console: https://console.aws.amazon.com/sagemaker/home?#/endpointConfig\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "   EndpointConfigName=endpoint_config_name,\n",
    "   ProductionVariants=[\n",
    "        {\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "            'InitialInstanceCount': 1,\n",
    "            'InstanceType': instance_type,\n",
    "            'EnableSSMAccess': False\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(endpoint_config_response)\n",
    "\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EndpointArn': 'arn:aws:sagemaker:us-west-2:571667364805:endpoint/sm-gec-aws', 'ResponseMetadata': {'RequestId': '96258e00-75ee-4ec6-9946-af03caf9ab79', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '96258e00-75ee-4ec6-9946-af03caf9ab79', 'content-type': 'application/x-amz-json-1.1', 'content-length': '78', 'date': 'Wed, 29 Nov 2023 05:22:21 GMT'}, 'RetryAttempts': 0}}\n",
      "Endpoint name: sm-gec-aws\n",
      "Endpoint arn:  arn:aws:sagemaker:us-west-2:571667364805:endpoint/sm-gec-aws\n"
     ]
    }
   ],
   "source": [
    "# create endpoint\n",
    "# ... endpoints console: https://console.aws.amazon.com/sagemaker/home?#/endpoints\n",
    "endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(endpoint_response)\n",
    "\n",
    "print('Endpoint name: {}'.format(endpoint_name))\n",
    "print('Endpoint arn:  {}'.format(endpoint_response['EndpointArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## **DEPRECATED** Trial 2: Use HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['HF_TRUST_REMOTE_CODE']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "-----!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.huggingface.model.HuggingFacePredictor at 0x7f9e67c66da0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "\n",
    "hub = {\n",
    "  'HF_MODEL_ID': 'dslim/bert-base-NER', #HF_MODEL_ID,\n",
    "  'HF_TASK':HF_TASK\n",
    "}\n",
    "\n",
    "hub = {\n",
    "  'HF_MODEL_ID': HF_MODEL_ID,\n",
    "  'HF_TASK':HF_TASK\n",
    "}\n",
    "\n",
    "# 763104351884.dkr.ecr.{region}.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\n",
    "\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=\"s3://project-langbot-models/gec-simple-model-hf-pytorch.tar.gz\",  # path to your trained SageMaker model\n",
    "   role=role,                                            # IAM role with permissions to create an endpoint\n",
    "   transformers_version=\"4.26\",                           # Transformers version used\n",
    "   pytorch_version=\"1.13\",                                # PyTorch version used\n",
    "   py_version='py39',                                    # Python version used\n",
    ")\n",
    "\n",
    "# # Using model hosted on HF\n",
    "# huggingface_model = HuggingFaceModel(\n",
    "#     transformers_version='4.26',\n",
    "#     pytorch_version='1.13',\n",
    "#     py_version='py39',\n",
    "#     env=hub,\n",
    "#     role=role,\n",
    "# )\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "huggingface_model.deploy(\n",
    "    endpoint_name = ENDPOINT_NAME,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge'\n",
    ")\n",
    "\n",
    "# # create Hugging Face Model Class\n",
    "# huggingface_model = HuggingFaceModel(\n",
    "#    model_data=\"s3://models/my-bert-model/model.tar.gz\",  # path to your trained SageMaker model\n",
    "#    role=role,                                            # IAM role with permissions to create an endpoint\n",
    "#    transformers_version=\"4.28\",                          # Transformers version used\n",
    "#    pytorch_version=\"2.0.0\",                              # PyTorch version used\n",
    "#    py_version='py39',                                    # Python version used\n",
    "# )\n",
    "\n",
    "# # deploy model to SageMaker Inference\n",
    "# predictor = huggingface_model.deploy(\n",
    "#    initial_instance_count=1,\n",
    "#    instance_type=instance_type\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# **DEPRECATED** Old Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install wandb tensorflow transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip show boto3\n",
    "!pip install boto3 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "# pip uninstall -y -q awscli\n",
    "# pip install awscli\n",
    "# pip install q -U sagemaker-ssh-helper\n",
    "# pip freeze | grep sagemaker-ssh-helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "arn:aws:iam::571667364805:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from utils import Config\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from model_utils import download_simple_model\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()\n",
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "s3 = boto3.resource('s3')\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "\n",
    "# The name of our algorithm -- i.e. the name of the inference container\n",
    "INFERENCE_ALGORITHM_NAME = \"sm-gec-aws\"\n",
    "ENDPOINT_NAME = \"sm-gec-aws\"\n",
    "IMAGE_URI_INFERENCE = (\n",
    "    f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{INFERENCE_ALGORITHM_NAME}:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Model Useless - Inference Endpoint Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Simple GEC - Inference Endpoint Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET = 'project-langbot-models'\n",
    "S3_DATA_KEY = f'gec_simple_model_{RUN_TO_DEPLOY}_weights.gz'\n",
    "PRETRAINED_MODEL_DATA = 's3://{}/{}'.format(S3_BUCKET, S3_DATA_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat model-useless/Dockerfile-useless-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize model-useless/code/useless-inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/model_gec_ram/deployment\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECR image fullname: 571667364805.dkr.ecr.us-west-2.amazonaws.com/sm-gec-aws:latest\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  15.87kB\n",
      "Step 1/8 : ARG REGION=us-west-2\n",
      "Step 2/8 : FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/tensorflow-inference:2.12.1-gpu-py310-cu118-ubuntu20.04-sagemaker\n",
      " ---> fde65ca56ee6\n",
      "Step 3/8 : ENV PATH=\"/opt/ml/code:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> be571fb97baa\n",
      "Step 4/8 : COPY /code /opt/ml/code\n",
      " ---> bf4d7aa16d06\n",
      "Step 5/8 : ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      " ---> Running in 7f49132b7e2a\n",
      "Removing intermediate container 7f49132b7e2a\n",
      " ---> 74b39635edaf\n",
      "Step 6/8 : ENV SAGEMAKER_PROGRAM useless-inference.py\n",
      " ---> Running in 954c5b329f81\n",
      "Removing intermediate container 954c5b329f81\n",
      " ---> 58defa8f975b\n",
      "Step 7/8 : RUN pip install --no-cache-dir --upgrade pip &&     pip install --no-cache-dir protobuf==3.20.* &&     pip install --no-cache-dir nvgpu smdebug transformers numba sentencepiece seqeval wandb\n",
      " ---> Running in 36fde70ee0a1\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.10/site-packages (23.1.2)\n",
      "Collecting pip\n",
      "  Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 47.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.1.2\n",
      "    Uninstalling pip-23.1.2:\n",
      "      Successfully uninstalled pip-23.1.2\n",
      "Successfully installed pip-23.3.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRequirement already satisfied: protobuf==3.20.* in /usr/local/lib/python3.10/site-packages (3.20.3)\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mCollecting nvgpu\n",
      "  Downloading nvgpu-0.10.0.tar.gz (8.4 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting smdebug\n",
      "  Downloading smdebug-1.0.34-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 123.5/123.5 kB 7.0 MB/s eta 0:00:00\n",
      "Collecting numba\n",
      "  Downloading numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 51.1 MB/s eta 0:00:00\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 214.1 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.16.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting ansi2html (from nvgpu)\n",
      "  Downloading ansi2html-1.8.0-py3-none-any.whl (16 kB)\n",
      "Collecting arrow (from nvgpu)\n",
      "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting flask (from nvgpu)\n",
      "  Downloading flask-3.0.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting flask-restful (from nvgpu)\n",
      "  Downloading Flask_RESTful-0.3.10-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting pandas (from nvgpu)\n",
      "  Downloading pandas-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting psutil (from nvgpu)\n",
      "  Downloading psutil-5.9.6-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from nvgpu) (2.27.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/site-packages (from nvgpu) (1.16.0)\n",
      "Collecting termcolor (from nvgpu)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting tabulate (from nvgpu)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting pynvml (from nvgpu)\n",
      "  Downloading pynvml-11.5.0-py3-none-any.whl (53 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 233.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf<=3.20.3,>=3.20.0 in /usr/local/lib/python3.10/site-packages (from smdebug) (3.20.3)\n",
      "Collecting numpy>=1.16.0 (from smdebug)\n",
      "  Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.2/61.2 kB 241.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from smdebug) (23.1)\n",
      "Requirement already satisfied: boto3>=1.10.32 in /usr/local/lib/python3.10/site-packages (from smdebug) (1.26.135)\n",
      "Collecting pyinstrument==3.4.2 (from smdebug)\n",
      "  Downloading pyinstrument-3.4.2-py2.py3-none-any.whl (83 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.3/83.3 kB 252.1 MB/s eta 0:00:00\n",
      "Collecting pyinstrument-cext>=0.2.2 (from pyinstrument==3.4.2->smdebug)\n",
      "  Downloading pyinstrument_cext-0.2.4.tar.gz (4.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (5.4.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 188.1 MB/s eta 0:00:00\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 151.8 MB/s eta 0:00:00\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba)\n",
      "  Downloading llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting scikit-learn>=0.21.3 (from seqeval)\n",
      "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting Click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.37.1-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from wandb) (67.7.2)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.135 in /usr/local/lib/python3.10/site-packages (from boto3>=1.10.32->smdebug) (1.29.135)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/site-packages (from boto3>=1.10.32->smdebug) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/site-packages (from boto3>=1.10.32->smdebug) (0.6.1)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->transformers)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.16.4->transformers)\n",
      "  Downloading typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->nvgpu) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->nvgpu) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/site-packages (from requests->nvgpu) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->nvgpu) (3.4)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn>=0.21.3->seqeval)\n",
      "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.4/60.4 kB 241.4 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.1.1 (from scikit-learn>=0.21.3->seqeval)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn>=0.21.3->seqeval)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/site-packages (from arrow->nvgpu) (2.8.2)\n",
      "Collecting types-python-dateutil>=2.8.10 (from arrow->nvgpu)\n",
      "  Downloading types_python_dateutil-2.8.19.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting Werkzeug>=3.0.0 (from flask->nvgpu)\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting Jinja2>=3.1.2 (from flask->nvgpu)\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 kB 292.9 MB/s eta 0:00:00\n",
      "Collecting itsdangerous>=2.1.2 (from flask->nvgpu)\n",
      "  Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting blinker>=1.6.2 (from flask->nvgpu)\n",
      "  Downloading blinker-1.7.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting aniso8601>=0.82 (from flask-restful->nvgpu)\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.8/52.8 kB 214.5 MB/s eta 0:00:00\n",
      "Collecting pytz (from flask-restful->nvgpu)\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->nvgpu)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 341.8/341.8 kB 319.1 MB/s eta 0:00:00\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from Jinja2>=3.1.2->flask->nvgpu)\n",
      "  Downloading MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Downloading smdebug-1.0.34-py2.py3-none-any.whl (280 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 280.1/280.1 kB 283.8 MB/s eta 0:00:00\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.9/7.9 MB 173.4 MB/s eta 0:00:00\n",
      "Downloading numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 226.6 MB/s eta 0:00:00\n",
      "Downloading wandb-0.16.0-py3-none-any.whl (2.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 268.4 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 216.5 MB/s eta 0:00:00\n",
      "Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 190.6/190.6 kB 184.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 311.7/311.7 kB 282.9 MB/s eta 0:00:00\n",
      "Downloading llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 MB 204.5 MB/s eta 0:00:00\n",
      "Downloading numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 228.8 MB/s eta 0:00:00\n",
      "Downloading psutil-5.9.6-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 283.6/283.6 kB 230.9 MB/s eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 773.9/773.9 kB 172.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 333.1 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.8/10.8 MB 208.6 MB/s eta 0:00:00\n",
      "Downloading sentry_sdk-1.37.1-py2.py3-none-any.whl (251 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 251.7/251.7 kB 294.0 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 220.0 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 149.7 MB/s eta 0:00:00\n",
      "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.4/66.4 kB 232.8 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading flask-3.0.0-py3-none-any.whl (99 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.7/99.7 kB 95.0 MB/s eta 0:00:00\n",
      "Downloading Flask_RESTful-0.3.10-py2.py3-none-any.whl (26 kB)\n",
      "Downloading pandas-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.3/12.3 MB 166.3 MB/s eta 0:00:00\n",
      "Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Downloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.4/166.4 kB 184.9 MB/s eta 0:00:00\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 150.0 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.2/302.2 kB 196.6 MB/s eta 0:00:00\n",
      "Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 502.5/502.5 kB 290.9 MB/s eta 0:00:00\n",
      "Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36.4/36.4 MB 181.7 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Downloading types_python_dateutil-2.8.19.14-py3-none-any.whl (9.4 kB)\n",
      "Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.7/226.7 kB 326.8 MB/s eta 0:00:00\n",
      "Downloading MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: nvgpu, seqeval, pyinstrument-cext\n",
      "  Building wheel for nvgpu (pyproject.toml): started\n",
      "  Building wheel for nvgpu (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for nvgpu: filename=nvgpu-0.10.0-py3-none-any.whl size=9543 sha256=278742fa2202e43b079edafd92c50c2f009841638518825960f2fddad1eaf3b5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4vkmj7uz/wheels/e2/c4/73/829a16caa9c3a3b6a0d16985c18d8c3b59c3812d1055552d8e\n",
      "  Building wheel for seqeval (pyproject.toml): started\n",
      "  Building wheel for seqeval (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=b020069b0f206c0310694824a33f31902217f94e40dfc580ed8901db06735d07\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4vkmj7uz/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
      "  Building wheel for pyinstrument-cext (pyproject.toml): started\n",
      "  Building wheel for pyinstrument-cext (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyinstrument-cext: filename=pyinstrument_cext-0.2.4-cp310-cp310-linux_x86_64.whl size=20434 sha256=fadfeae099910e0f9635c36b17450fcbbe8980cb14e437582534ff1cd0f7ff3c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-4vkmj7uz/wheels/0f/8b/7a/5f7fd1dd6d3cbb3d350d4c832c5e2f962687749f6d67d573a6\n",
      "Successfully built nvgpu seqeval pyinstrument-cext\n",
      "Installing collected packages: types-python-dateutil, sentencepiece, pytz, pyinstrument-cext, appdirs, aniso8601, tzdata, typing-extensions, tqdm, threadpoolctl, termcolor, tabulate, smmap, setproctitle, sentry-sdk, safetensors, regex, pynvml, pyinstrument, psutil, numpy, MarkupSafe, llvmlite, joblib, itsdangerous, fsspec, filelock, docker-pycreds, Click, blinker, ansi2html, Werkzeug, scipy, pandas, numba, Jinja2, huggingface-hub, gitdb, arrow, tokenizers, scikit-learn, GitPython, flask, wandb, transformers, seqeval, flask-restful, smdebug, nvgpu\n",
      "Successfully installed Click-8.1.7 GitPython-3.1.40 Jinja2-3.1.2 MarkupSafe-2.1.3 Werkzeug-3.0.1 aniso8601-9.0.1 ansi2html-1.8.0 appdirs-1.4.4 arrow-1.3.0 blinker-1.7.0 docker-pycreds-0.4.0 filelock-3.13.1 flask-3.0.0 flask-restful-0.3.10 fsspec-2023.10.0 gitdb-4.0.11 huggingface-hub-0.19.4 itsdangerous-2.1.2 joblib-1.3.2 llvmlite-0.41.1 numba-0.58.1 numpy-1.26.2 nvgpu-0.10.0 pandas-2.1.3 psutil-5.9.6 pyinstrument-3.4.2 pyinstrument-cext-0.2.4 pynvml-11.5.0 pytz-2023.3.post1 regex-2023.10.3 safetensors-0.4.1 scikit-learn-1.3.2 scipy-1.11.4 sentencepiece-0.1.99 sentry-sdk-1.37.1 seqeval-1.2.2 setproctitle-1.3.3 smdebug-1.0.34 smmap-5.0.1 tabulate-0.9.0 termcolor-2.3.0 threadpoolctl-3.2.0 tokenizers-0.15.0 tqdm-4.66.1 transformers-4.35.2 types-python-dateutil-2.8.19.14 typing-extensions-4.8.0 tzdata-2023.3 wandb-0.16.0\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 36fde70ee0a1\n",
      " ---> 90cb67e56a78\n",
      "Step 8/8 : RUN pip freeze\n",
      " ---> Running in 0ef32f4c0346\n",
      "aniso8601==9.0.1\n",
      "ansi2html==1.8.0\n",
      "appdirs==1.4.4\n",
      "arrow==1.3.0\n",
      "awscli==1.27.135\n",
      "blinker==1.7.0\n",
      "boto3==1.26.135\n",
      "botocore==1.29.135\n",
      "certifi==2023.5.7\n",
      "charset-normalizer==2.0.12\n",
      "click==8.1.7\n",
      "colorama==0.4.4\n",
      "Cython==0.29.30\n",
      "docker-pycreds==0.4.0\n",
      "docutils==0.16\n",
      "falcon==3.0.1\n",
      "filelock==3.13.1\n",
      "Flask==3.0.0\n",
      "Flask-RESTful==0.3.10\n",
      "fsspec==2023.10.0\n",
      "gevent==21.12.0\n",
      "gitdb==4.0.11\n",
      "GitPython==3.1.40\n",
      "greenlet==1.1.3.post0\n",
      "grpcio==1.46.3\n",
      "gunicorn==20.1.0\n",
      "huggingface-hub==0.19.4\n",
      "idna==3.4\n",
      "itsdangerous==2.1.2\n",
      "Jinja2==3.1.2\n",
      "jmespath==1.0.1\n",
      "joblib==1.3.2\n",
      "llvmlite==0.41.1\n",
      "MarkupSafe==2.1.3\n",
      "numba==0.58.1\n",
      "numpy==1.26.2\n",
      "nvgpu==0.10.0\n",
      "packaging==23.1\n",
      "pandas==2.1.3\n",
      "protobuf==3.20.3\n",
      "psutil==5.9.6\n",
      "pyasn1==0.5.0\n",
      "pyinstrument==3.4.2\n",
      "pyinstrument-cext==0.2.4\n",
      "pynvml==11.5.0\n",
      "python-dateutil==2.8.2\n",
      "pytz==2023.3.post1\n",
      "PyYAML==5.4.1\n",
      "regex==2023.10.3\n",
      "requests==2.27.1\n",
      "rsa==4.7.2\n",
      "s3transfer==0.6.1\n",
      "safetensors==0.4.1\n",
      "scikit-learn==1.3.2\n",
      "scipy==1.11.4\n",
      "sentencepiece==0.1.99\n",
      "sentry-sdk==1.37.1\n",
      "seqeval==1.2.2\n",
      "setproctitle==1.3.3\n",
      "six==1.16.0\n",
      "smdebug==1.0.34\n",
      "smmap==5.0.1\n",
      "tabulate==0.9.0\n",
      "tensorflow-serving-api-gpu==2.12.1\n",
      "termcolor==2.3.0\n",
      "threadpoolctl==3.2.0\n",
      "tokenizers==0.15.0\n",
      "tqdm==4.66.1\n",
      "transformers==4.35.2\n",
      "types-python-dateutil==2.8.19.14\n",
      "typing_extensions==4.8.0\n",
      "tzdata==2023.3\n",
      "urllib3==1.26.15\n",
      "wandb==0.16.0\n",
      "Werkzeug==3.0.1\n",
      "zope.event==4.6\n",
      "zope.interface==6.0\n",
      "Removing intermediate container 0ef32f4c0346\n",
      " ---> 3fe420cbf772\n",
      "Successfully built 3fe420cbf772\n",
      "Successfully tagged sm-gec-aws:latest\n",
      "The push refers to repository [571667364805.dkr.ecr.us-west-2.amazonaws.com/sm-gec-aws]\n",
      "\n",
      "\u001b[1Bc0cab016: Preparing \n",
      "\u001b[1B6f7fda01: Preparing \n",
      "\u001b[1B6cfb3167: Preparing \n",
      "\u001b[1B7575fae0: Preparing \n",
      "\u001b[1B7fc65bc2: Preparing \n",
      "\u001b[1B65e0fd45: Preparing \n",
      "\u001b[1Be2696bb8: Preparing \n",
      "\u001b[1B1becb391: Preparing \n",
      "\u001b[1Be6e6bad0: Preparing \n",
      "\u001b[1B7ac9d18b: Preparing \n",
      "\u001b[1Bedcde126: Preparing \n",
      "\u001b[1B7ebab9f5: Preparing \n",
      "\u001b[1B28bfd596: Preparing \n",
      "\u001b[1B77cc4166: Preparing \n",
      "\u001b[1B1cfdf474: Preparing \n",
      "\u001b[1B659733f0: Preparing \n",
      "\u001b[1B0bf54747: Preparing \n",
      "\u001b[1B89de37f2: Preparing \n",
      "\u001b[1Bc1df018a: Preparing \n",
      "\u001b[1B2d1cfdaf: Preparing \n",
      "\u001b[1B0423988f: Preparing \n",
      "\u001b[1B7c4de29d: Preparing \n",
      "\u001b[1Becc50864: Preparing \n",
      "\u001b[1B7e3e56a9: Preparing \n",
      "\u001b[1B7e76bf1b: Preparing \n",
      "\u001b[1Ba70ab1cd: Preparing \n",
      "\u001b[1B48c52364: Preparing \n",
      "\u001b[24Bfc65bc2: Retrying in 1 second   [28A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2KRetrying in 15 seconds \u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[24A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2KEOF\n"
     ]
    }
   ],
   "source": [
    "! cd model-useless && sh build_and_push.sh {INFERENCE_ALGORITHM_NAME} Dockerfile-useless-inference && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/model_gec_ram/deployment\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "-------!"
     ]
    }
   ],
   "source": [
    "from sagemaker import Model\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "import sagemaker as sage\n",
    "\n",
    "sess = sage.Session()\n",
    "\n",
    "instance_type = \"ml.p2.xlarge\"\n",
    "\n",
    "estimator = TensorFlowModel(\n",
    "    model_data=PRETRAINED_MODEL_DATA,\n",
    "    source_dir=\"model-useless/code\",\n",
    "    role=role, \n",
    "    entry_point=\"useless_inference.py\",\n",
    "    image_uri=IMAGE_URI_INFERENCE,\n",
    "    sagemaker_session=sess)\n",
    "\n",
    "# estimator = Model(\n",
    "#     image_uri=IMAGE_URI_INFERENCE,\n",
    "#     model_data=PRETRAINED_MODEL_DATA,\n",
    "#     role=role,\n",
    "#     source_dir=\"model-useless/code\",\n",
    "#     entry_point=\"useless-inference.py\",\n",
    "#     sagemaker_session=sess\n",
    "# )\n",
    "\n",
    "# ssh_wrapper = SSHEstimatorWrapper.create(estimator, connection_wait_time_seconds=0, local_user_id=local_user_id, log_to_stdout=True)\n",
    "\n",
    "# ssh_wrapper = SSHModelWrapper.create(estimator, connection_wait_time_seconds=0)\n",
    "\n",
    "# deploy the model\n",
    "predictor = estimator.deploy(1, instance_type, endpoint_name=ENDPOINT_NAME, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports And Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow sagemaker wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simple GEC specific constants\n",
    "RUN_TO_DEPLOY = '3emhdbgu' # The wandb run from which to get model weights\n",
    "S3_BUCKET = 'project-langbot-models'\n",
    "S3_DATA_KEY = f'gec_simple_model_{RUN_TO_DEPLOY}_weights.gz'\n",
    "PRETRAINED_MODEL_DATA = 's3://{}/{}'.format(S3_BUCKET, S3_DATA_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Push Model Weights To S3 From WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload the model weights from WandB to S3 bucket\n",
    "def copy_weights_to_s3(runid, download=True):\n",
    "    main_args = Config()\n",
    "    run_ref = main_args.SIMPLE_MODEL_RUNS[runid]\n",
    "    weights_filename = 'model_weights.gz'\n",
    "    if download:\n",
    "        download_simple_model(run_ref, weights_filename)\n",
    "    # Push the downloaded weights to s3 bucket\n",
    "    role = get_execution_role()\n",
    "    s3.meta.client.upload_file(f'downloads/{weights_filename}', S3_BUCKET, S3_DATA_KEY)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found <Run langbot/langbot_gec_plain_top_performers/3emhdbgu (finished)> to load artifact from\n",
      "Downloading model_weights.gz\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "copy_weights_to_s3(RUN_TO_DEPLOY, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://project-langbot-models/gec_simple_model_3emhdbgu_weights.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "s3.Object(bucket_name='project-langbot-models', key='gec_simple_model_3emhdbgu_weights.gz')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm model weights are there on S3\n",
    "print(PRETRAINED_MODEL_DATA)\n",
    "s3.Bucket(S3_BUCKET).Object(S3_DATA_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DockerFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Based on\n",
      "# https://github.com/aws/amazon-sagemaker-examples/blob/main/advanced_functionality/tensorflow_iris_byom/tensorflow_BYOM_iris.ipynb\n",
      "\n",
      "ARG REGION=us-west-2\n",
      "\n",
      "# SageMaker TF image for INFERENCE\n",
      "FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/tensorflow-inference:2.12.1-gpu-py310-cu118-ubuntu20.04-sagemaker\n",
      "\n",
      "ENV PATH=\"/opt/ml/code:${PATH}\"\n",
      "\n",
      "# /opt/ml and all subdirectories are utilized by SageMaker, we use the /code subdirectory to store our user code.\n",
      "COPY /gec /opt/ml/code\n",
      "\n",
      "# this environment variable is used by the SageMaker TensorFlow container to determine our user code directory.\n",
      "ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      "\n",
      "# this environment variable is used by the SageMaker Tensorflow container to determine our program entry point\n",
      "# for training and serving.\n",
      "# For more information: https://github.com/aws/sagemaker-tensorflow-training-toolkit\n",
      "ENV SAGEMAKER_PROGRAM gec-simple-inference.py\n",
      "\n",
      "RUN pip install --no-cache-dir --upgrade pip && \\\n",
      "    pip install --no-cache-dir protobuf==3.20.* && \\\n",
      "    pip install --no-cache-dir nvgpu smdebug transformers numba sentencepiece seqeval wandb\n",
      "\n",
      "RUN pip freeze\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat model-simple/Dockerfile-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference script - gec-simple-inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pygmentize model-simple/gec/gec-simple-inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build and Push Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/model_gec_ram/deployment\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECR image fullname: 571667364805.dkr.ecr.us-west-2.amazonaws.com/sm-gec-aws:latest\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  72.19kB\n",
      "Step 1/8 : ARG REGION=us-west-2\n",
      "Step 2/8 : FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/tensorflow-inference:2.12.1-gpu-py310-cu118-ubuntu20.04-sagemaker\n",
      " ---> fde65ca56ee6\n",
      "Step 3/8 : ENV PATH=\"/opt/ml/code:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> be571fb97baa\n",
      "Step 4/8 : COPY /gec /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> 1385fbe8293b\n",
      "Step 5/8 : ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> 2da6d66a094c\n",
      "Step 6/8 : ENV SAGEMAKER_PROGRAM gec-simple-inference.py\n",
      " ---> Using cache\n",
      " ---> b7f1e2dd5f29\n",
      "Step 7/8 : RUN pip install --no-cache-dir --upgrade pip &&     pip install --no-cache-dir protobuf==3.20.* &&     pip install --no-cache-dir nvgpu smdebug transformers numba sentencepiece seqeval wandb\n",
      " ---> Using cache\n",
      " ---> 8cf03d760ac1\n",
      "Step 8/8 : RUN pip freeze\n",
      " ---> Using cache\n",
      " ---> 31ba8edbafa5\n",
      "Successfully built 31ba8edbafa5\n",
      "Successfully tagged sm-gec-aws:latest\n",
      "The push refers to repository [571667364805.dkr.ecr.us-west-2.amazonaws.com/sm-gec-aws]\n",
      "\n",
      "\u001b[1Bca6a4b8a: Preparing \n",
      "\u001b[1B03acd6de: Preparing \n",
      "\u001b[1B6cfb3167: Preparing \n",
      "\u001b[1B7575fae0: Preparing \n",
      "\u001b[1B7fc65bc2: Preparing \n",
      "\u001b[1B65e0fd45: Preparing \n",
      "\u001b[1Be2696bb8: Preparing \n",
      "\u001b[1B1becb391: Preparing \n",
      "\u001b[1Be6e6bad0: Preparing \n",
      "\u001b[1B7ac9d18b: Preparing \n",
      "\u001b[1Bedcde126: Preparing \n",
      "\u001b[1B7ebab9f5: Preparing \n",
      "\u001b[1B28bfd596: Preparing \n",
      "\u001b[1B77cc4166: Preparing \n",
      "\u001b[1B1cfdf474: Preparing \n",
      "\u001b[1B659733f0: Preparing \n",
      "\u001b[1B0bf54747: Preparing \n",
      "\u001b[1B89de37f2: Preparing \n",
      "\u001b[1Bc1df018a: Preparing \n",
      "\u001b[1B2d1cfdaf: Preparing \n",
      "\u001b[1B0423988f: Preparing \n",
      "\u001b[1B7c4de29d: Preparing \n",
      "\u001b[1Becc50864: Preparing \n",
      "\u001b[1B7e3e56a9: Preparing \n",
      "\u001b[1B7e76bf1b: Preparing \n",
      "\u001b[1Ba70ab1cd: Preparing \n",
      "\u001b[1B48c52364: Preparing \n",
      "\u001b[24Bfc65bc2: Retrying in 1 second   [26A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[24A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[24A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[24A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[24A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[24A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[24A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[24A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[24A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[24A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[24A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[27A\u001b[2K\u001b[24A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2K\u001b[28A\u001b[2K\u001b[25A\u001b[2K\u001b[28A\u001b[2K\u001b[26A\u001b[2KEOF\n"
     ]
    }
   ],
   "source": [
    "! cd model-simple && sh build_and_push.sh {INFERENCE_ALGORITHM_NAME} Dockerfile-inference && cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Inference Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/model_gec_ram/deployment\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "--------!"
     ]
    }
   ],
   "source": [
    "from sagemaker import Model\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "import sagemaker as sage\n",
    "\n",
    "sess = sage.Session()\n",
    "\n",
    "# instance_type = \"ml.m5.xlarge\" # no GPU, will trigger an error\n",
    "# instance_type = \"ml.g4dn.xlarge\"\n",
    "instance_type = \"ml.p2.xlarge\"\n",
    "\n",
    "# endpoint_deployment_name = \"sm-gec-aws\"\n",
    "# estimator = TensorFlowModel(\n",
    "#     model_data=PRETRAINED_MODEL_DATA, \n",
    "#     source_dir=\"model-simple/gec\",\n",
    "#     role=role, \n",
    "#     entry_point=\"gec-simple-inference.py\", \n",
    "#     image_uri=IMAGE_URI_INFERENCE)\n",
    "\n",
    "estimator = Model(\n",
    "    image_uri=IMAGE_URI_INFERENCE,\n",
    "    model_data=PRETRAINED_MODEL_DATA,\n",
    "    role=role,\n",
    "    source_dir=\"model-simple/gec\",\n",
    "    entry_point=\"gec-simple-inference.py\",\n",
    "    sagemaker_session=sess,  # not local session anymore\n",
    "    #                   predictor_cls=None,\n",
    "    #                   env=None,\n",
    "    #                   name=None,\n",
    "    #                   vpc_config=None,\n",
    "    #                   enable_network_isolation=False,\n",
    "    #                   model_kms_key=None,\n",
    "    #                   image_config=None,\n",
    "    #                   code_location=None,\n",
    "    #                   container_log_level=20,\n",
    "    #                   dependencies=None,\n",
    "    #                   git_config=None\n",
    ")\n",
    "\n",
    "# ssh_wrapper = SSHEstimatorWrapper.create(estimator, connection_wait_time_seconds=0, local_user_id=local_user_id, log_to_stdout=True)\n",
    "\n",
    "# ssh_wrapper = SSHModelWrapper.create(estimator, connection_wait_time_seconds=0)\n",
    "\n",
    "# deploy the model\n",
    "predictor = estimator.deploy(1, instance_type, endpoint_name=ENDPOINT_NAME, wait=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GEC With Explanation - Inference Endpoint Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre trained NER model should be pickled and uploaded to the correct S3 bucket as a tarball. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "BUCKET = \"project-langbot-models\"\n",
    "KEY = \"gec_cows_l2h_small.gz\"\n",
    "PRETRAINED_MODEL_DATA = \"s3://{}/{}\".format(BUCKET, KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://project-langbot-models/gec_cows_l2h_small.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "s3.Object(bucket_name='project-langbot-models', key='gec_cows_l2h_small.gz')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(PRETRAINED_MODEL_DATA)\n",
    "boto3.Session().resource(\"s3\").Bucket(BUCKET).Object(KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Based on\n",
      "# https://github.com/awslabs/amazon-sagemaker-examples/master/advanced_functionality/pytorch_extending_our_containers/pytorch_extending_our_containers.ipynb\n",
      "\n",
      "ARG REGION=us-west-2\n",
      "\n",
      "# SageMaker PyTorch image for INFERENCE\n",
      "FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/pytorch-inference:1.12.1-gpu-py38-cu113-ubuntu20.04-sagemaker\n",
      "\n",
      "ENV PATH=\"/opt/ml/code:${PATH}\"\n",
      "\n",
      "# /opt/ml and all subdirectories are utilized by SageMaker, we use the /code subdirectory to store our user code.\n",
      "COPY /gec /opt/ml/code\n",
      "\n",
      "# this environment variable is used by the SageMaker PyTorch container to determine our user code directory.\n",
      "ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      "\n",
      "# this environment variable is used by the SageMaker PyTorch container to determine our program entry point\n",
      "# for training and serving.\n",
      "# For more information: https://github.com/aws/sagemaker-pytorch-container\n",
      "ENV SAGEMAKER_PROGRAM gec-inference.py\n",
      "\n",
      "RUN pip install --no-cache-dir --upgrade pip && \\\n",
      "    pip install --no-cache-dir numba==0.53.1 protobuf==3.20.* && pip install --no-cache-dir simpletransformers==0.64.3 nvgpu smdebug\n",
      "\n",
      "RUN pip freeze\n"
     ]
    }
   ],
   "source": [
    "!cat model1/container/Dockerfile-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference script - gec-inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "JSON_CONTENT_TYPE = \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "logger.setLevel(logging.DEBUG)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m====================== Running stuff ============================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33minside model_fn, model_dir= \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_dir\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mDevice Type: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model_loc = \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/gec_cows_l2h_small.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(model_loc):\u001b[37m\u001b[39;49;00m\n",
      "        logging.error(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mMissing model file \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_loc\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model = \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_loc, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:  \u001b[37m# open a text file\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        model = pickle.load(f) \u001b[37m# serialize the list\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# TODO: How to add GPU support for inferencing? Looks like NER model does not have .to() method.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#       Also do we need GPU for inference?\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# model.to(device)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mGEC model loaded into device \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdevice\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(data, model):\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mGot input Data: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdata\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    prediction, _ = model.predict([data])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m prediction\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type=JSON_CONTENT_TYPE):\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mserialized_input_data object: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mserialized_input_data\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == JSON_CONTENT_TYPE:\u001b[37m\u001b[39;49;00m\n",
      "        input_data = json.loads(serialized_input_data)\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33minput_data object: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00minput_data\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m input_data[\u001b[33m'\u001b[39;49;00m\u001b[33mline\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in Accept: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction, content_type):\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mprediction object before: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mprediction\u001b[33m}\u001b[39;49;00m\u001b[33m, type: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mtype\u001b[39;49;00m(prediction)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Transform predictions to JSON\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    prediction_result = {\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33moutput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: prediction\u001b[37m\u001b[39;49;00m\n",
      "    }\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mprediction_result object: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mprediction_result\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m prediction_result\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize container/gec/gec-inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build and Push Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECR image fullname: 571667364805.dkr.ecr.us-west-2.amazonaws.com/sm-gec-aws:latest\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  17.92kB\n",
      "Step 1/8 : ARG REGION=us-west-2\n",
      "Step 2/8 : FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/pytorch-inference:1.12.1-gpu-py38-cu113-ubuntu20.04-sagemaker\n",
      " ---> cc486ae090f7\n",
      "Step 3/8 : ENV PATH=\"/opt/ml/code:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> ed6c9f68b5de\n",
      "Step 4/8 : COPY /gec /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> 923b5fcc86b5\n",
      "Step 5/8 : ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> fd25ac165e77\n",
      "Step 6/8 : ENV SAGEMAKER_PROGRAM gec-inference.py\n",
      " ---> Using cache\n",
      " ---> 58bc40662a62\n",
      "Step 7/8 : RUN pip install --no-cache-dir --upgrade pip &&     pip install --no-cache-dir numba==0.53.1 protobuf==3.20.* && pip install --no-cache-dir simpletransformers==0.64.3 nvgpu smdebug\n",
      " ---> Using cache\n",
      " ---> 27f4195ba81c\n",
      "Step 8/8 : RUN pip freeze\n",
      " ---> Using cache\n",
      " ---> ec46c7b89984\n",
      "Successfully built ec46c7b89984\n",
      "Successfully tagged sm-gec-aws:latest\n",
      "The push refers to repository [571667364805.dkr.ecr.us-west-2.amazonaws.com/sm-gec-aws]\n",
      "\n",
      "\u001b[1B74712a0c: Preparing \n",
      "\u001b[1B10414572: Preparing \n",
      "\u001b[1B37eeb897: Preparing \n",
      "\u001b[1B424ed6bf: Preparing \n",
      "\u001b[1Bb74bcd62: Preparing \n",
      "\u001b[1Bc6ec42e4: Preparing \n",
      "\u001b[1B44dc3edd: Preparing \n",
      "\u001b[1Ba78a3211: Preparing \n",
      "\u001b[1Bca7e3783: Preparing \n",
      "\u001b[1B07982a5f: Preparing \n",
      "\u001b[1B276eda72: Preparing \n",
      "\u001b[1Bb1b6f05e: Preparing \n",
      "\u001b[1B061b424e: Preparing \n",
      "\u001b[1B6e954844: Preparing \n",
      "\u001b[1B4676e227: Preparing \n",
      "\u001b[1B0fc30092: Preparing \n",
      "\u001b[1B8305c9a9: Preparing \n",
      "\u001b[1Bff3531fd: Preparing \n",
      "\u001b[1Bc617cad4: Preparing \n",
      "\u001b[1Bd6065936: Preparing \n",
      "\u001b[1Bf92cc97b: Preparing \n",
      "\u001b[1B0809d74c: Preparing \n",
      "\u001b[1Bf7fa3a1f: Preparing \n",
      "\u001b[1B4b2240de: Preparing \n",
      "\u001b[1Ba902ec50: Preparing \n",
      "\u001b[1B4828897a: Preparing \n",
      "\u001b[1B9aa8404f: Preparing \n",
      "\u001b[1B2b3d5595: Preparing \n",
      "\u001b[1B5186b826: Preparing \n",
      "\u001b[1B521d62fe: Preparing \n",
      "\u001b[1Bd36776dc: Preparing \n",
      "\u001b[1Bce4c0976: Preparing \n",
      "\u001b[1B7375cb04: Preparing \n",
      "\u001b[1B672e1e8b: Preparing \n",
      "\u001b[1B9e761223: Preparing \n",
      "\u001b[33B24ed6bf: Retrying in 1 second   [32A\u001b[2K\u001b[35A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2KEOF\n"
     ]
    }
   ],
   "source": [
    "! cd container && sh build_and_push.sh {INFERENCE_ALGORITHM_NAME} Dockerfile-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Inference Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "-------!"
     ]
    }
   ],
   "source": [
    "from sagemaker import Model\n",
    "\n",
    "import sagemaker as sage\n",
    "\n",
    "sess = sage.Session()\n",
    "\n",
    "# instance_type = \"ml.m5.xlarge\" # no GPU, will trigger an error\n",
    "# instance_type = \"ml.g4dn.xlarge\"\n",
    "instance_type = \"ml.p2.xlarge\"\n",
    "\n",
    "# endpoint_deployment_name = \"sm-gec-aws\"\n",
    "\n",
    "estimator = Model(\n",
    "    image_uri=IMAGE_URI_INFERENCE,\n",
    "    model_data=PRETRAINED_MODEL_DATA,\n",
    "    role=role,\n",
    "    source_dir=\"container/gec\",\n",
    "    entry_point=\"gec-inference.py\",\n",
    "    sagemaker_session=sess,  # not local session anymore\n",
    "    #                   predictor_cls=None,\n",
    "    #                   env=None,\n",
    "    #                   name=None,\n",
    "    #                   vpc_config=None,\n",
    "    #                   enable_network_isolation=False,\n",
    "    #                   model_kms_key=None,\n",
    "    #                   image_config=None,\n",
    "    #                   code_location=None,\n",
    "    #                   container_log_level=20,\n",
    "    #                   dependencies=None,\n",
    "    #                   git_config=None\n",
    ")\n",
    "\n",
    "# deploy the model\n",
    "predictor = estimator.deploy(1, instance_type, endpoint_name=ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Optional cleanup of the create endpoint\n",
    "The created endpoint can be deleted with the code below.\n",
    "\n",
    "This part represent the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '9150081a-5ab9-42a7-bfa1-28533741574d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '9150081a-5ab9-42a7-bfa1-28533741574d',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Mon, 04 Dec 2023 19:24:39 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client(\"sagemaker\")\n",
    "response = client.describe_endpoint_config(EndpointConfigName=ENDPOINT_NAME)\n",
    "model_name = response[\"ProductionVariants\"][0][\"ModelName\"]\n",
    "client.delete_endpoint(EndpointName=ENDPOINT_NAME)\n",
    "# client.delete_endpoint_config(EndpointConfigName=ENDPOINT_NAME)\n",
    "# client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
