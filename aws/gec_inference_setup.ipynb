{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring GEC Inference Endpoint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre trained NER model should be pickled and uploaded to the correct S3 bucket as a tarball. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=1)\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "BUCKET = \"project-langbot-models\"\n",
    "KEY = \"gec_cows_l2h_small.gz\"\n",
    "PRETRAINED_MODEL_DATA = \"s3://{}/{}\".format(BUCKET, KEY)\n",
    "# The name of our algorithm -- i.e. the name of the inference container\n",
    "INFERENCE_ALGORITHM_NAME = \"sm-gec-aws\"\n",
    "ENDPOINT_NAME = \"sm-gec-aws\"\n",
    "\n",
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "\n",
    "IMAGE_URI_INFERENCE = (\n",
    "    f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{INFERENCE_ALGORITHM_NAME}:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Check Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://project-langbot-models/gec_cows_l2h_small.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "s3.Object(bucket_name='project-langbot-models', key='gec_cows_l2h_small.gz')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(PRETRAINED_MODEL_DATA)\n",
    "boto3.Session().resource(\"s3\").Bucket(BUCKET).Object(KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Docker Container For Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright 2017-2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n",
      "# may not use this file except in compliance with the License. A copy of\n",
      "# the License is located at\n",
      "#\n",
      "#     http://aws.amazon.com/apache2.0/\n",
      "#\n",
      "# or in the \"license\" file accompanying this file. This file is\n",
      "# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n",
      "# ANY KIND, either express or implied. See the License for the specific\n",
      "# language governing permissions and limitations under the License.\n",
      "\n",
      "# For more information on creating a Dockerfile\n",
      "# https://docs.docker.com/compose/gettingstarted/#step-2-create-a-dockerfile\n",
      "# https://github.com/awslabs/amazon-sagemaker-examples/master/advanced_functionality/pytorch_extending_our_containers/pytorch_extending_our_containers.ipynb\n",
      "\n",
      "ARG REGION=us-west-2\n",
      "\n",
      "# SageMaker PyTorch image for INFERENCE\n",
      "FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/pytorch-inference:1.12.1-gpu-py38-cu113-ubuntu20.04-sagemaker\n",
      "\n",
      "ENV PATH=\"/opt/ml/code:${PATH}\"\n",
      "\n",
      "# /opt/ml and all subdirectories are utilized by SageMaker, we use the /code subdirectory to store our user code.\n",
      "COPY /gec /opt/ml/code\n",
      "\n",
      "# this environment variable is used by the SageMaker PyTorch container to determine our user code directory.\n",
      "ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      "\n",
      "# this environment variable is used by the SageMaker PyTorch container to determine our program entry point\n",
      "# for training and serving.\n",
      "# For more information: https://github.com/aws/sagemaker-pytorch-container\n",
      "ENV SAGEMAKER_PROGRAM gec-inference.py\n",
      "\n",
      "RUN pip install --no-cache-dir --upgrade pip && \\\n",
      "    pip install --no-cache-dir numba==0.53.1 protobuf==3.20.* && pip install --no-cache-dir simpletransformers==0.64.3 nvgpu smdebug\n",
      "\n",
      "RUN pip freeze\n"
     ]
    }
   ],
   "source": [
    "!cat container/Dockerfile-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Inference script - gec-inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "JSON_CONTENT_TYPE = \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "logger.setLevel(logging.DEBUG)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m====================== Running stuff ============================\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33minside model_fn, model_dir= \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_dir\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    device = \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mDevice Type: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(device))\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model_loc = \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/gec_cows_l2h_small.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(model_loc):\u001b[37m\u001b[39;49;00m\n",
      "        logging.error(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mMissing model file \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_loc\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    model = \u001b[34mNone\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_loc, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:  \u001b[37m# open a text file\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        model = pickle.load(f) \u001b[37m# serialize the list\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# TODO: How to add GPU support for inferencing? Looks like NER model does not have .to() method.\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m#       Also do we need GPU for inference?\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[37m# model.to(device)\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mGEC model loaded into device \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdevice\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(data, model):\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mGot input Data: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdata\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    prediction, _ = model.predict([data])\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m prediction\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type=JSON_CONTENT_TYPE):\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mserialized_input_data object: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mserialized_input_data\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m content_type == JSON_CONTENT_TYPE:\u001b[37m\u001b[39;49;00m\n",
      "        input_data = json.loads(serialized_input_data)\u001b[37m\u001b[39;49;00m\n",
      "        logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33minput_data object: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00minput_data\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mreturn\u001b[39;49;00m input_data[\u001b[33m'\u001b[39;49;00m\u001b[33mline\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in Accept: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction, content_type):\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mprediction object before: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mprediction\u001b[33m}\u001b[39;49;00m\u001b[33m, type: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mtype\u001b[39;49;00m(prediction)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[37m# Transform predictions to JSON\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "    prediction_result = {\u001b[37m\u001b[39;49;00m\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33moutput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: prediction\u001b[37m\u001b[39;49;00m\n",
      "    }\u001b[37m\u001b[39;49;00m\n",
      "\u001b[37m\u001b[39;49;00m\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mprediction_result object: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mprediction_result\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m prediction_result\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize container/gec/gec-inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build and Push Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECR image fullname: 571667364805.dkr.ecr.us-west-2.amazonaws.com/sm-gec-aws:latest\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  17.92kB\n",
      "Step 1/8 : ARG REGION=us-west-2\n",
      "Step 2/8 : FROM 763104351884.dkr.ecr.$REGION.amazonaws.com/pytorch-inference:1.12.1-gpu-py38-cu113-ubuntu20.04-sagemaker\n",
      " ---> cc486ae090f7\n",
      "Step 3/8 : ENV PATH=\"/opt/ml/code:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> ed6c9f68b5de\n",
      "Step 4/8 : COPY /gec /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> 923b5fcc86b5\n",
      "Step 5/8 : ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      " ---> Using cache\n",
      " ---> fd25ac165e77\n",
      "Step 6/8 : ENV SAGEMAKER_PROGRAM gec-inference.py\n",
      " ---> Using cache\n",
      " ---> 58bc40662a62\n",
      "Step 7/8 : RUN pip install --no-cache-dir --upgrade pip &&     pip install --no-cache-dir numba==0.53.1 protobuf==3.20.* && pip install --no-cache-dir simpletransformers==0.64.3 nvgpu smdebug\n",
      " ---> Using cache\n",
      " ---> 27f4195ba81c\n",
      "Step 8/8 : RUN pip freeze\n",
      " ---> Using cache\n",
      " ---> ec46c7b89984\n",
      "Successfully built ec46c7b89984\n",
      "Successfully tagged sm-gec-aws:latest\n",
      "The push refers to repository [571667364805.dkr.ecr.us-west-2.amazonaws.com/sm-gec-aws]\n",
      "\n",
      "\u001b[1B74712a0c: Preparing \n",
      "\u001b[1B10414572: Preparing \n",
      "\u001b[1B37eeb897: Preparing \n",
      "\u001b[1B424ed6bf: Preparing \n",
      "\u001b[1Bb74bcd62: Preparing \n",
      "\u001b[1Bc6ec42e4: Preparing \n",
      "\u001b[1B44dc3edd: Preparing \n",
      "\u001b[1Ba78a3211: Preparing \n",
      "\u001b[1Bca7e3783: Preparing \n",
      "\u001b[1B07982a5f: Preparing \n",
      "\u001b[1B276eda72: Preparing \n",
      "\u001b[1Bb1b6f05e: Preparing \n",
      "\u001b[1B061b424e: Preparing \n",
      "\u001b[1B6e954844: Preparing \n",
      "\u001b[1B4676e227: Preparing \n",
      "\u001b[1B0fc30092: Preparing \n",
      "\u001b[1B8305c9a9: Preparing \n",
      "\u001b[1Bff3531fd: Preparing \n",
      "\u001b[1Bc617cad4: Preparing \n",
      "\u001b[1Bd6065936: Preparing \n",
      "\u001b[1Bf92cc97b: Preparing \n",
      "\u001b[1B0809d74c: Preparing \n",
      "\u001b[1Bf7fa3a1f: Preparing \n",
      "\u001b[1B4b2240de: Preparing \n",
      "\u001b[1Ba902ec50: Preparing \n",
      "\u001b[1B4828897a: Preparing \n",
      "\u001b[1B9aa8404f: Preparing \n",
      "\u001b[1B2b3d5595: Preparing \n",
      "\u001b[1B5186b826: Preparing \n",
      "\u001b[1B521d62fe: Preparing \n",
      "\u001b[1Bd36776dc: Preparing \n",
      "\u001b[1Bce4c0976: Preparing \n",
      "\u001b[1B7375cb04: Preparing \n",
      "\u001b[1B672e1e8b: Preparing \n",
      "\u001b[1B9e761223: Preparing \n",
      "\u001b[33B24ed6bf: Retrying in 1 second   [32A\u001b[2K\u001b[35A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[33A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[32A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2K\u001b[34A\u001b[2K\u001b[35A\u001b[2K\u001b[36A\u001b[2KEOF\n"
     ]
    }
   ],
   "source": [
    "! cd container && sh build_and_push.sh {INFERENCE_ALGORITHM_NAME} Dockerfile-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Inference Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "-------!"
     ]
    }
   ],
   "source": [
    "from sagemaker import Model\n",
    "\n",
    "import sagemaker as sage\n",
    "\n",
    "sess = sage.Session()\n",
    "\n",
    "# instance_type = \"ml.m5.xlarge\" # no GPU, will trigger an error\n",
    "# instance_type = \"ml.g4dn.xlarge\"\n",
    "instance_type = \"ml.p2.xlarge\"\n",
    "\n",
    "# endpoint_deployment_name = \"sm-gec-aws\"\n",
    "\n",
    "estimator = Model(\n",
    "    image_uri=IMAGE_URI_INFERENCE,\n",
    "    model_data=PRETRAINED_MODEL_DATA,\n",
    "    role=role,\n",
    "    source_dir=\"container/gec\",\n",
    "    entry_point=\"gec-inference.py\",\n",
    "    sagemaker_session=sess,  # not local session anymore\n",
    "    #                   predictor_cls=None,\n",
    "    #                   env=None,\n",
    "    #                   name=None,\n",
    "    #                   vpc_config=None,\n",
    "    #                   enable_network_isolation=False,\n",
    "    #                   model_kms_key=None,\n",
    "    #                   image_config=None,\n",
    "    #                   code_location=None,\n",
    "    #                   container_log_level=20,\n",
    "    #                   dependencies=None,\n",
    "    #                   git_config=None\n",
    ")\n",
    "\n",
    "# deploy the model\n",
    "predictor = estimator.deploy(1, instance_type, endpoint_name=ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT r.read().decode(): {\n",
      "  \"output\": [\n",
      "    [\n",
      "      {\n",
      "        \"Hola\": \"O\"\n",
      "      },\n",
      "      {\n",
      "        \"Como\": \"O\"\n",
      "      },\n",
      "      {\n",
      "        \"estas?\": \"O\"\n",
      "      }\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "sm_client = sess.sagemaker_runtime_client\n",
    "# endpoint_name = \"sm-gec-aws\"\n",
    "response = sm_client.invoke_endpoint(\n",
    "    EndpointName=ENDPOINT_NAME,\n",
    "    ContentType=\"application/json\",\n",
    "    Body=json.dumps({\"line\":\"Hola Como estas?\"}),\n",
    ")\n",
    "\n",
    "r = response[\"Body\"]\n",
    "print(\"RESULT r.read().decode():\", r.read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Optional cleanup of the create endpoint\n",
    "The created endpoint can be deleted with the code below.\n",
    "\n",
    "This part represent the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '3c23ebd8-0982-43c3-a797-b41f04869fa1',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '3c23ebd8-0982-43c3-a797-b41f04869fa1',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '0',\n",
       "   'date': 'Tue, 14 Nov 2023 07:54:17 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client(\"sagemaker\")\n",
    "response = client.describe_endpoint_config(EndpointConfigName=ENDPOINT_NAME)\n",
    "model_name = response[\"ProductionVariants\"][0][\"ModelName\"]\n",
    "client.delete_endpoint(EndpointName=ENDPOINT_NAME)\n",
    "# client.delete_endpoint_config(EndpointConfigName=ENDPOINT_NAME)\n",
    "# client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
